{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from git import Repo\n",
    "from langchain.text_splitter import Language\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone Github Repository  \n",
    "https://github.com/VaibhavBomle/Llama-2-Project-on-CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\GenerativeAI-Basics\\\\Project-Generative-AI\\\\source-code-analysis-project-GenAl\\\\research'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir test_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<git.repo.base.Repo 'c:\\\\GenerativeAI-Basics\\\\Project-Generative-AI\\\\source-code-analysis-project-GenAl\\\\research\\\\test_repo\\\\.git'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_path = \"test_repo/\"\n",
    "Repo.clone_from(\"https://github.com/VaibhavBomle/Llama-2-Project-on-CPU\",to_path=repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = \"test_repo/\"\n",
    "\n",
    "loader = GenericLoader.from_filesystem(repo_path+'/src',\n",
    "                                       glob=\"**/*\",\n",
    "                                       suffixes=[\".py\"],\n",
    "                                       parser=LanguageParser(language=Language.PYTHON,parser_threshold=300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents  = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='DEFAULT_SYSTEM_PROMPT=\"\"\"\\\\\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. \\nYour answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. \\nPlease ensure that your responses are socially unbiased and positive in nature.\\nIf a question does not make any sense, or is not factually coherent, explain why instead of \\nanswering something not correct. If you don\\'t know the answer to a question,\\nplease don\\'t share false information.\"\"\"\\n\\n\\n\\n#CUSTOM_SYSTEM_PROMPT=\"You are an advanced assistant that provides translation from English to Hindi\"\\nCUSTOM_SYSTEM_PROMPT=\"You are an advanced assistant that provides summarization given any book name\"\\n\\n\\n\\n\\ntemplate=\"\"\"Use the following pieces of information to answer the user\\'s question.\\nIf you dont know the answer just say you know, don\\'t try to make up an answer.\\n\\nContext:{context}\\nQuestion:{question}\\n\\nOnly return the helpful answer below and nothing else\\nHelpful answer\\n\"\"\"', metadata={'source': 'test_repo\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from langchain import PromptTemplate\\nfrom langchain import LLMChain\\nfrom langchain.llms import CTransformers\\nfrom src.helper import *\\n\\n\\n\\n\\nB_INST,E_INST = \"[INST]\",\"[/INST]\"\\nB_SYS,E_SYS = \"<<SYS>>\\\\n\", \"\\\\n<</SYS>>\\\\n\\\\n\"\\n\\ninstruction = \"Convert the following text from English to Hindi: \\\\n\\\\n {text}\"\\n\\nSYSTEM_PROMPT = B_SYS + DEFAULT_SYSTEM_PROMPT + E_SYS\\ntemplate = B_INST + SYSTEM_PROMPT + instruction + E_INST\\n\\nprompt = PromptTemplate(template=template,input_variables=[\"text\"])\\n\\n\\nllm = CTransformers(model=\\'C:\\\\GenerativeAI-Basics\\\\Project-Generative-AI\\\\Llama-2-Project-on-CPU\\\\model\\\\llama-2-7b-chat.ggmlv3.q4_0.bin\\',\\n                    model_type=\\'llama\\',\\n                    config={\\'max_new_tokens\\':128,\\n                            \\'temperature\\':0.01}\\n                            )\\n\\nLLMChain = LLMChain(prompt=prompt,llm=llm)\\n\\nprint(LLMChain.run(\"How are you?\"))\\n\\n\\n# Download locally\\n# from huggingface_hub import hf_hub_download\\n# hf_hub_download(\\n#     repo_id=\"TheBloke/Llama-2-7B-Chat-GGML\",\\n#     filename=\"llama-2-7b-chat.ggmlv3.q8_0.bin\",\\n#     local_dir=\"./models\"\\n# )\\n\\n', metadata={'source': 'test_repo\\\\src\\\\run_local.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from langchain import PromptTemplate\\nfrom langchain import LLMChain\\nfrom langchain.llms import CTransformers\\nfrom src.helper import *\\n\\n\\n\\n\\nB_INST,E_INST = \"[INST]\",\"[/INST]\"\\nB_SYS,E_SYS = \"<<SYS>>\\\\n\", \"\\\\n<</SYS>>\\\\n\\\\n\"\\n\\ninstruction = \"Give proper summery of he text: \\\\n\\\\n {text}\"\\n\\nSYSTEM_PROMPT = B_SYS + CUSTOM_SYSTEM_PROMPT + E_SYS\\ntemplate = B_INST + SYSTEM_PROMPT + instruction + E_INST\\n\\nprompt = PromptTemplate(template=template,input_variables=[\"text\"])\\n\\n\\nllm = CTransformers(model=\\'C:\\\\GenerativeAI-Basics\\\\Project-Generative-AI\\\\Llama-2-Project-on-CPU\\\\model\\\\llama-2-7b-chat.ggmlv3.q4_0.bin\\',\\n                    model_type=\\'llama\\',\\n                    config={\\'max_new_tokens\\':128,\\n                            \\'temperature\\':0.01}\\n                            )\\n\\nLLMChain = LLMChain(prompt=prompt,llm=llm)\\n\\n#print(LLMChain.run(\"How are you?\"))\\n\\n#print(LLMChain.run(\"I am Curious to learn Artificial intelligence\"))\\n\\nprint(LLMChain.run(\"Harry Porter\"))\\n\\n\\n', metadata={'source': 'test_repo\\\\src\\\\run_local1.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\src\\\\__init__.py', 'language': <Language.PYTHON: 'python'>})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunkings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_splitter = RecursiveCharacterTextSplitter.from_language(language=Language.PYTHON,\n",
    "                                                                 chunk_size = 2000,\n",
    "                                                                 chunk_overlap=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "texts = documents_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embadding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"OPENAI_API_KEY\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=OpenAIEmbeddings(disallowed_special=())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knowledge base (vector DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectordb = Chroma.from_documents(texts, embedding=embeddings, persist_directory='./data')\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = ChatOpenAI(model_name=\"gpt-4\")\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryMemory(llm=llm, memory_key = \"chat_history\", return_messages=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(llm, retriever=vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\":3}), memory=memory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q & A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is PromptTemplate?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = qa(question)\n",
    "print(result['answer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
